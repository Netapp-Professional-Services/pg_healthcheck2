= Cassandra Health Check Generation
:toc: left
:toclevels: 3

You are an expert developer and AI code generator for the pg_healthcheck2 multi-database monitoring framework.

Your task: Generate a complete health check solution as a JSON plan for {{ plugin_name }}.

Your output MUST be a single, valid JSON object with NO additional text, explanations, or markdown formatting.

== Core Architecture Contract

Every health check module MUST implement these two functions:

=== Function 1: Weight Declaration

[source,python]
----
def get_weight():
    """Returns the importance score for this module (1-10)."""
    return 7  # Choose based on severity
----

*Weight Guidelines:*
- *1-3 (Low):* Informational checks, statistics, version info
- *4-6 (Medium):* Performance concerns, configuration recommendations
- *7-8 (High):* Significant operational issues, resource exhaustion
- *9-10 (Critical):* Data corruption risks, service availability threats

=== Function 2: Main Check Execution

[source,python]
----
def run_check_name(connector, settings):
    """
    Performs the health check analysis.
    
    Args:
        connector: Database connector with execute_query() method
        settings: Dictionary of configuration settings (main config, not connector settings)
    
    Returns:
        tuple: (asciidoc_report_string, structured_data_dict)
    """
    adoc_content = []  # MUST be a list
    structured_data = {}  # MUST be a dict
    
    # ... check logic ...
    
    return "\n".join(adoc_content), structured_data
----

== Using Shared Helper Functions (RECOMMENDED)

The framework provides reusable helper functions in `plugins.common.check_helpers` to reduce boilerplate and ensure consistency:

[source,python]
----
from plugins.common.check_helpers import (
    require_ssh,              # Check if SSH is available
    format_check_header,      # Standard check header formatting
    format_recommendations,   # Standard recommendation formatting
    safe_execute_query       # Safe query execution with error handling
)
----

=== Helper Function: require_ssh()

Replaces manual SSH configuration checking:

[source,python]
----
# ✅ NEW PATTERN (Recommended):
ssh_ok, skip_msg, skip_data = require_ssh(connector, "nodetool commands")
if not ssh_ok:
    adoc_content.append(skip_msg)
    structured_data["check_result"] = skip_data
    return "\n".join(adoc_content), structured_data

# ✅ OLD PATTERN (Still supported):
connector_settings = getattr(connector, 'settings', {})
ssh_configured = (
    'ssh_host' in connector_settings and
    'ssh_user' in connector_settings and
    ('ssh_key_file' in connector_settings or 'ssh_password' in connector_settings)
)
if not ssh_configured:
    # ... manual skip message ...
----

**Benefits:**
- ✅ Consistent error messages
- ✅ Less boilerplate code
- ✅ Standardized skip data structure

=== Helper Function: format_check_header()

Creates standard check headers with optional SSH requirement notice:

[source,python]
----
# ✅ NEW PATTERN:
adoc_content = format_check_header(
    "Node Status Analysis (Nodetool)",
    "Checking cluster node health using `nodetool status`.",
    requires_ssh=True  # Adds SSH requirement notice
)

# Equivalent to:
adoc_content = [
    "=== Node Status Analysis (Nodetool)",
    "",
    "Checking cluster node health using `nodetool status`.",
    "",
    "[NOTE]",
    "====",
    "This check requires SSH access to the database server.",
    "===="
]
----

=== Helper Function: format_recommendations()

Formats recommendation lists consistently:

[source,python]
----
# ✅ NEW PATTERN:
recommendations = [
    "SSH to node {address} and check logs",
    "Verify network connectivity with 'nodetool gossipinfo'",
    "Check disk space with 'df -h'"
]
adoc_content.extend(format_recommendations(recommendations))

# Generates:
# ==== Recommendations
# [TIP]
# ====
# * SSH to node {address} and check logs
# * Verify network connectivity with 'nodetool gossipinfo'
# * Check disk space with 'df -h'
# ====
----

=== Helper Function: safe_execute_query()

Wraps query execution with consistent error handling:

[source,python]
----
# ✅ NEW PATTERN:
query = get_nodetool_status_query(connector)
success, formatted, raw = safe_execute_query(connector, query, "Nodetool status")

if not success:
    adoc_content.append(formatted)
    structured_data["node_status"] = {"status": "error", "data": raw}
    return "\n".join(adoc_content), structured_data

# Equivalent to:
try:
    formatted, raw = connector.execute_query(query, return_raw=True)
    if "[ERROR]" in formatted:
        # ... error handling ...
except Exception as e:
    # ... exception handling ...
----

**Benefits:**
- ✅ Consistent error message format
- ✅ Automatic exception handling
- ✅ Returns success boolean for easy checking

== Database Interaction Rules (CRITICAL)

*NEVER* use raw cursors or direct database access. *ALWAYS* use the connector's high-level API:

[source,python]
----
# CORRECT: Use connector.execute_query()
formatted, raw = connector.execute_query(query, return_raw=True)

# WRONG: Never do this
cursor = connector.cursor()  # ❌ FORBIDDEN
cursor.execute(query)        # ❌ FORBIDDEN
----

*The connector provides:*
- `connector.execute_query(query, params=None, return_raw=False)` - Core query method
- `connector.version_info` - Dictionary with 'version_string' and 'major_version'
- `connector.get_db_metadata()` - Returns dict with 'version' and 'db_name'
- `connector.settings` - Dictionary with connection settings including SSH config

*Always check version_info existence before using:*
[source,python]
----
if hasattr(connector, 'version_info'):
    major = connector.version_info.get('major_version', 0)
    if major >= 4:
        # Use Cassandra 4.x features
----

== Cassandra-Specific Guidance (CRITICAL)

=== Two Query Types: CQL vs Nodetool

The Cassandra connector supports two distinct operation types:

1. **CQL Queries** - Standard SELECT statements against system tables
2. **Nodetool Commands** - JSON-based requests executed via SSH for operational metrics

[IMPORTANT]
====
Choose the RIGHT tool for the job:

*Use CQL for:*
- Schema information (keyspaces, tables, columns)
- Replication strategies
- Basic topology (datacenters, racks)
- Node addresses and versions

*Use Nodetool for:*
- Node status (Up/Down, Normal/Leaving/Joining)
- Load and disk usage
- Compaction statistics
- Thread pool metrics
- Operational health
====

=== SSH Configuration for Nodetool Commands (CRITICAL)

[CRITICAL]
====
**Nodetool commands require SSH access to Cassandra nodes.**

When generating checks that use nodetool, you MUST check for SSH configuration.

**RECOMMENDED:** Use the `require_ssh()` helper function from `plugins.common.check_helpers`.

**Alternative:** Check `connector.settings` (NOT the `settings` parameter) for SSH configuration.

**Why:** The `settings` parameter contains the main config.yaml settings. The `connector.settings` contains the actual connection settings including SSH configuration passed during connector initialization.
====

==== RECOMMENDED SSH Check Pattern (Use Helpers):

[source,python]
----
from plugins.common.check_helpers import require_ssh, format_check_header

def run_nodetool_check(connector, settings):
    """
    Check that requires nodetool/SSH access.
    
    Args:
        connector: Database connector with execute_query() and settings
        settings: Main application settings (NOT used for SSH check)
    
    Returns:
        tuple: (asciidoc_report_string, structured_data_dict)
    """
    # ✅ RECOMMENDED: Use helper function
    adoc_content = format_check_header(
        "Check Title (Nodetool)",
        "Description of check using nodetool commands.",
        requires_ssh=True
    )
    structured_data = {}
    
    # ✅ RECOMMENDED: Check SSH with helper
    ssh_ok, skip_msg, skip_data = require_ssh(connector, "nodetool commands")
    if not ssh_ok:
        adoc_content.append(skip_msg)
        structured_data["check_result"] = skip_data
        return "\n".join(adoc_content), structured_data
    
    # Proceed with nodetool command...
    try:
        query = get_nodetool_command_query(connector)
        success, formatted, raw = safe_execute_query(connector, query, "Nodetool command")
        
        if not success:
            adoc_content.append(formatted)
            structured_data["result"] = {"status": "error", "data": raw}
            return "\n".join(adoc_content), structured_data
        
        # Handle results...
----

==== Alternative SSH Check Pattern (Manual):

[source,python]
----
def run_nodetool_check(connector, settings):
    """Check that requires nodetool/SSH access."""
    adoc_content = [
        "=== Check Title (Nodetool)",
        "",
        "Description of check using nodetool commands."
    ]
    structured_data = {}
    
    # ✅ ALTERNATIVE: Check connector.settings manually
    connector_settings = getattr(connector, 'settings', {})
    ssh_configured = (
        'ssh_host' in connector_settings and
        'ssh_user' in connector_settings and
        ('ssh_key_file' in connector_settings or 'ssh_password' in connector_settings)
    )
    
    if not ssh_configured:
        adoc_content.append("[IMPORTANT]\n====\n"
                          "This check requires SSH access to a Cassandra node.\n"
                          "Configure ssh_host, ssh_user, and ssh_key_file or ssh_password "
                          "in your settings.\n====\n")
        structured_data["check_result"] = {
            "status": "skipped",
            "reason": "SSH not configured"
        }
        return "\n".join(adoc_content), structured_data
    
    # Proceed with nodetool command...
----

==== WRONG SSH Check Pattern (NEVER USE THIS):

[source,python]
----
def run_nodetool_check(connector, settings):
    # ❌ WRONG: Checking settings parameter instead of connector.settings
    ssh_configured = (
        'ssh_host' in settings and
        'ssh_user' in settings and
        ('ssh_key_file' in settings or 'ssh_password' in settings)
    )
    # This will ALWAYS fail because settings doesn't contain SSH config!
----

==== When SSH Check is NOT Needed:

CQL-only checks don't require SSH and should NOT include the SSH check:

[source,python]
----
def run_cql_check(connector, settings):
    """
    Check using only CQL queries.
    No SSH configuration needed.
    """
    adoc_content = ["=== CQL-Based Check", ""]
    structured_data = {}
    
    # No SSH check needed - direct CQL query
    query = get_cql_query(connector)
    formatted, raw = connector.execute_query(query, return_raw=True)
    
    # Process results...
----

=== CQL Query Pattern (End-to-End)

==== Step 1: Query Library File

*File:* `plugins/cassandra/utils/qrylib/qry_keyspace_replication.py`

[CRITICAL]
====
**File Naming Convention:**

Query files MUST be named `qry_{check_name}.py` to match the check file name.

Examples:
- Check: `node_status_check.py` → Query: `qry_node_status.py`
- Check: `compaction_pending_tasks.py` → Query: `qry_compaction_pending_tasks.py`
- Check: `keyspace_replication_check.py` → Query: `qry_keyspace_replication.py`

**Why:** This prevents file overwrites when multiple checks are generated. Each check gets its own unique query file.
====

[source,python]
----
"""Keyspace replication queries for Cassandra."""

__all__ = [
    'get_keyspace_replication_query'
]

def get_keyspace_replication_query(connector):
    """
    Returns query for keyspace replication strategies.
    
    Args:
        connector: Cassandra connector instance
    
    Returns:
        str: CQL SELECT statement
    """
    return """
    SELECT keyspace_name, replication, durable_writes
    FROM system_schema.keyspaces;
    """
----

[CRITICAL]
====
CQL does NOT support `WHERE keyspace_name NOT IN (...)` for filtering.
You MUST filter system keyspaces in Python code, not in the query.
====

==== Step 2: Check Module Using CQL

*File:* `plugins/cassandra/checks/keyspace_replication_check.py`

[source,python]
----
from plugins.cassandra.utils.qrylib.qry_keyspace_replication import get_keyspace_replication_query
from plugins.common.check_helpers import format_check_header, format_recommendations, safe_execute_query

def get_weight():
    return 7

def run_keyspace_replication_check(connector, settings):
    """
    Analyzes keyspace replication strategies.
    
    Args:
        connector: Database connector with execute_query() method
        settings: Dictionary of configuration settings
    
    Returns:
        tuple: (asciidoc_report_string, structured_data_dict)
    """
    adoc_content = format_check_header(
        "Keyspace Replication Strategy Analysis",
        "Analyzing replication strategies for all user keyspaces."
    )
    structured_data = {}
    
    try:
        query = get_keyspace_replication_query(connector)
        success, formatted, raw = safe_execute_query(connector, query, "Keyspace replication query")
        
        if not success:
            adoc_content.append(formatted)
            structured_data["replication"] = {"status": "error", "data": raw}
            return "\n".join(adoc_content), structured_data
        
        # Filter out system keyspaces in Python
        system_keyspaces = {'system', 'system_schema', 'system_traces', 
                           'system_auth', 'system_distributed', 'system_views'}
        user_keyspaces = [ks for ks in raw 
                          if ks.get('keyspace_name') not in system_keyspaces]
        
        if not user_keyspaces:
            adoc_content.append("[NOTE]\n====\nNo user keyspaces found.\n====\n")
            structured_data["replication"] = {"status": "success", "data": []}
            return "\n".join(adoc_content), structured_data
        
        # Analyze replication strategies
        simple_strategy_keyspaces = []
        for ks in user_keyspaces:
            replication = ks.get('replication', {})
            if 'SimpleStrategy' in replication.get('class', ''):
                simple_strategy_keyspaces.append(ks['keyspace_name'])
        
        if simple_strategy_keyspaces:
            adoc_content.append("[WARNING]\n====\n"
                              f"**{len(simple_strategy_keyspaces)} keyspace(s)** "
                              "using SimpleStrategy (not recommended for production).\n"
                              "====\n")
            adoc_content.append(formatted)
            
            recommendations = [
                "Plan maintenance window to alter keyspaces to NetworkTopologyStrategy",
                "Calculate appropriate replication factor per datacenter (typically RF=3)",
                "After altering replication, run 'nodetool repair' to ensure data consistency"
            ]
            adoc_content.extend(format_recommendations(recommendations))
        else:
            adoc_content.append("[NOTE]\n====\n"
                              "All user keyspaces use NetworkTopologyStrategy.\n"
                              "====\n")
        
        structured_data["replication"] = {
            "status": "success",
            "data": user_keyspaces,
            "simple_strategy_count": len(simple_strategy_keyspaces)
        }
        
    except Exception as e:
        error_msg = f"[ERROR]\n====\nReplication check failed: {str(e)}\n====\n"
        adoc_content.append(error_msg)
        structured_data["replication"] = {"status": "error", "details": str(e)}
    
    return "\n".join(adoc_content), structured_data
----

=== CQL Query Construction Rules (CRITICAL)

[CRITICAL]
====
**NEVER use bind variables (?) in Cassandra CQL queries.**

The Cassandra connector does NOT support parameterized queries with `?` placeholders.
Instead, use direct string interpolation when the values come from trusted sources.
====

==== WRONG - Do Not Use Bind Variables:

[source,python]
----
# ❌ This will fail with "Invalid amount of bind variables"
def get_tables_query(connector):
    return """
    SELECT table_name
    FROM system_schema.tables
    WHERE keyspace_name = ?;
    """

# ❌ This will fail - connector doesn't pass params correctly
tables_query = get_tables_query(connector)
formatted, raw = connector.execute_query(tables_query, [ks_name], return_raw=True)
----

==== CORRECT - Use String Interpolation:

[source,python]
----
# ✅ Direct string interpolation is safe when values come from system tables
def get_tables_query(connector, keyspace_name):
    """
    Returns query for tables in a specific keyspace.
    
    Args:
        connector: Cassandra connector instance
        keyspace_name: Name of the keyspace (from system tables - safe)
    
    Returns:
        str: CQL SELECT statement with keyspace name embedded
    """
    return f"""
    SELECT table_name
    FROM system_schema.tables
    WHERE keyspace_name = '{keyspace_name}';
    """

# ✅ No params passed to execute_query
tables_query = get_tables_query(connector, ks_name)
formatted, raw = connector.execute_query(tables_query, return_raw=True)
----

==== Why String Interpolation is Safe Here:

1. **Keyspace names come from system tables** - They are validated by Cassandra itself
2. **No user input** - Values are from `SELECT * FROM system_schema.keyspaces`
3. **CQL naming rules** - Keyspace/table names have strict constraints (alphanumeric + underscore)
4. **Connector limitation** - The connector's `execute_query()` doesn't properly support bind variables

==== General Pattern for All CQL Queries:

[source,python]
----
# Query function signature includes all filter values
def get_something_query(connector, filter_value):
    """Returns CQL query with filter_value directly interpolated."""
    return f"SELECT * FROM system.table WHERE field = '{filter_value}';"

# Check module calls with the value
query = get_something_query(connector, value_from_system_table)
formatted, raw = connector.execute_query(query, return_raw=True)
----

[IMPORTANT]
====
**Key Rule:** Query functions should accept filter values as parameters and embed them directly in the SQL string. NEVER return queries with `?` placeholders.
====


=== Query File Structure and Integration Testing

==== Required: __all__ Declaration

Every query file MUST start with an `__all__` list explicitly declaring its public query functions:

[source,python]
----
"""Schema-related queries for Cassandra."""

__all__ = [
    'get_keyspaces_query',
    'get_tables_query',
    'get_row_count_query'
]

def get_keyspaces_query(connector):
    """Returns query for all keyspaces."""
    return "SELECT ..."

def get_tables_query(connector, keyspace_name='system'):
    """Returns query for tables - has default for integration testing."""
    return f"SELECT ... WHERE keyspace_name = '{keyspace_name}';"
----

**Benefits of __all__:**
- ✅ Makes public API explicit
- ✅ Enables better IDE support and type checking
- ✅ Separates public functions from private helpers
- ✅ Standard Python convention

==== Integration Test Rules

The integration test framework validates query functions automatically. Follow these rules:

1. **List all public functions in __all__** - Only functions in `__all__` will be tested
2. **Private helpers start with underscore** - Functions like `_build_filter()` are ignored
3. **Provide defaults for parameters** - Functions with extra parameters need defaults:

[source,python]
----
# ✅ GOOD: Has defaults, can be tested
def get_tables_query(connector, keyspace_name='system'):
    """Default to 'system' keyspace for integration testing."""
    return f"SELECT table_name FROM system_schema.tables WHERE keyspace_name = '{keyspace_name}';"

# ❌ PROBLEMATIC: No defaults, will be skipped in integration tests
def get_row_count_query(connector, keyspace_name, table_name):
    """This will be skipped - no test values provided."""
    return f"SELECT COUNT(*) FROM {keyspace_name}.{table_name};"

# ✅ BETTER: Add reasonable defaults
def get_row_count_query(connector, keyspace_name='system', table_name='local'):
    """Defaults allow integration testing without setup."""
    return f"SELECT COUNT(*) FROM {keyspace_name}.{table_name};"
----

**Default Value Guidelines:**
- Use `'system'` keyspace (always exists in Cassandra)
- Use `'local'` table (exists in system keyspace)
- Choose defaults that work on a fresh installation
- Document why the default was chosen


=== Nodetool Query Pattern (End-to-End)

==== Step 1: Query Library File

*File:* `plugins/cassandra/utils/qrylib/qry_node_status.py`

[CRITICAL]
====
**File Naming Convention:**

Query files MUST be named `qry_{check_name}.py` to match the check file name.

For nodetool checks:
- Check: `node_status_check.py` → Query: `qry_node_status.py`
- Check: `compaction_pending_tasks.py` → Query: `qry_compaction_pending_tasks.py`

**Never use generic names like `nodetool_queries.py` - this will cause overwrites!**
====

[source,python]
----
"""Node status queries for Cassandra (nodetool)."""

__all__ = [
    'get_nodetool_status_query'
]

import json

def get_nodetool_status_query(connector):
    """
    Returns JSON request for 'nodetool status' command.
    
    Args:
        connector: Cassandra connector instance
    
    Returns:
        str: JSON string with operation and command
    """
    return json.dumps({
        "operation": "nodetool",
        "command": "status"
    })
----

==== Step 2: Check Module Using Nodetool (WITH HELPERS)

*File:* `plugins/cassandra/checks/node_status_check.py`

[source,python]
----
from plugins.cassandra.utils.qrylib.qry_node_status import get_nodetool_status_query
from plugins.common.check_helpers import (
    require_ssh,
    format_check_header,
    format_recommendations,
    safe_execute_query
)

def get_weight():
    return 9  # Critical - node availability

def run_node_status_check(connector, settings):
    """
    Performs node status analysis using nodetool.
    
    Args:
        connector: Database connector with execute_query() method
        settings: Dictionary of configuration settings
    
    Returns:
        tuple: (asciidoc_report_string, structured_data_dict)
    """
    # Initialize with formatted header
    adoc_content = format_check_header(
        "Node Status Analysis (Nodetool)",
        "Checking cluster node health using `nodetool status`.",
        requires_ssh=True
    )
    structured_data = {}
    
    # Check SSH availability using helper
    ssh_ok, skip_msg, skip_data = require_ssh(connector, "nodetool commands")
    if not ssh_ok:
        adoc_content.append(skip_msg)
        structured_data["node_status"] = skip_data
        return "\n".join(adoc_content), structured_data
    
    # Execute check using safe helper
    query = get_nodetool_status_query(connector)
    success, formatted, raw = safe_execute_query(connector, query, "Nodetool status")
    
    if not success:
        adoc_content.append(formatted)
        structured_data["node_status"] = {"status": "error", "data": raw}
        return "\n".join(adoc_content), structured_data
    
    # Analyze results
    nodes = raw if isinstance(raw, list) else []
    
    if not nodes:
        adoc_content.append("[NOTE]\n====\nNo node data returned.\n====\n")
        structured_data["node_status"] = {"status": "success", "data": []}
        return "\n".join(adoc_content), structured_data
    
    # Find unhealthy nodes
    unhealthy_nodes = []
    for node in nodes:
        status = node.get('status', 'U')
        state = node.get('state', 'N')
        if status != 'U' or state != 'N':
            unhealthy_nodes.append(node)
    
    # Format results
    if unhealthy_nodes:
        adoc_content.append(
            f"[CRITICAL]\n====\n"
            f"**{len(unhealthy_nodes)} node(s)** not in UN "
            "(Up/Normal) state. This poses availability risk.\n"
            "====\n"
        )
        adoc_content.append(formatted)
        
        # Use helper for recommendations
        recommendations = [
            "SSH to affected nodes and check Cassandra logs for errors.",
            "Verify network connectivity: ensure nodes can communicate with each other.",
            "Check resources: verify disk space, memory, and CPU are not exhausted.",
            "If node is down, attempt restart: 'systemctl restart cassandra'"
        ]
        adoc_content.extend(format_recommendations(recommendations))
        
        status_result = "critical"
    else:
        adoc_content.append(
            f"[NOTE]\n====\n"
            f"All {len(nodes)} node(s) are healthy (UN state).\n"
            "====\n"
        )
        adoc_content.append(formatted)
        status_result = "success"
    
    structured_data["node_status"] = {
        "status": status_result,
        "data": nodes,
        "total_nodes": len(nodes),
        "unhealthy_count": len(unhealthy_nodes)
    }
    
    return "\n".join(adoc_content), structured_data
----

==== Understanding Nodetool Output Structure

When you execute a nodetool command, the connector automatically parses the output:

*nodetool status* returns list of dicts:
[source,python]
----
[
    {
        'datacenter': 'datacenter1',
        'status': 'U',           # U=Up, D=Down
        'state': 'N',            # N=Normal, L=Leaving, J=Joining, M=Moving
        'address': '127.0.0.1',
        'load': '108.45 KB',
        'tokens': 256,
        'owns_effective_percent': 100.0,
        'host_id': 'aaa-bbb-ccc',
        'rack': 'rack1'
    }
]
----

*nodetool compactionstats* returns dict:
[source,python]
----
{
    'pending_tasks': 15,
    'active_compactions': [
        {
            'compaction_id': 'abc123',
            'keyspace': 'my_keyspace',
            'table': 'my_table',
            'completed': 50000000,
            'total': 100000000,
            'unit': 'bytes',
            'type': 'Compaction'
        }
    ]
}
----

*nodetool tpstats* returns list of dicts:
[source,python]
----
[
    {
        'pool_name': 'ReadStage',
        'active': 0,
        'pending': 0,
        'completed': 12345,
        'blocked': 0,
        'all_time_blocked': 0
    }
]
----


=== Shell Command Pattern (End-to-End)

==== When to Use Shell Commands

Use shell commands for system-level metrics that aren't available through CQL or nodetool:

*Common Use Cases:*
- Disk space per mount point: `df -h`
- Memory usage: `free -m`
- CPU metrics: `mpstat`, `iostat`
- Process information: `ps aux | grep cassandra`
- Network statistics: `netstat -s`, `ss -s`
- File system checks: `du -sh /var/lib/cassandra`
- Log file analysis: `tail -n 100 /var/log/cassandra/system.log`

[IMPORTANT]
====
**Shell commands require SSH access, just like nodetool commands.**

Always include SSH configuration validation using the `require_ssh()` helper.
====

==== Step 1: Query Library File

*File:* `plugins/cassandra/utils/qrylib/qry_disk_usage.py`

[source,python]
----
"""Disk usage queries for Cassandra (shell commands)."""

__all__ = [
    'get_disk_usage_query',
    'get_cassandra_data_dir_usage_query'
]

import json

def get_disk_usage_query(connector):
    """
    Returns JSON request for disk usage via 'df -h' command.
    
    Args:
        connector: Cassandra connector instance
    
    Returns:
        str: JSON string with operation and command
    """
    return json.dumps({
        "operation": "shell",
        "command": "df -h"
    })

def get_cassandra_data_dir_usage_query(connector, data_dir='/var/lib/cassandra'):
    """
    Returns JSON request for Cassandra data directory size.
    
    Args:
        connector: Cassandra connector instance
        data_dir: Path to Cassandra data directory (default: /var/lib/cassandra)
    
    Returns:
        str: JSON string with operation and command
    """
    return json.dumps({
        "operation": "shell",
        "command": f"du -sh {data_dir}"
    })
----

==== Step 2: Check Module Using Shell Commands

*File:* `plugins/cassandra/checks/disk_usage_check.py`

[source,python]
----
from plugins.cassandra.utils.qrylib.qry_disk_usage import get_disk_usage_query
from plugins.common.check_helpers import (
    require_ssh,
    format_check_header,
    format_recommendations,
    safe_execute_query
)

def get_weight():
    return 7  # High - disk space issues are critical

def run_disk_usage_check(connector, settings):
    """
    Analyzes disk usage on the Cassandra server using shell commands.
    
    Args:
        connector: Database connector with execute_query() method
        settings: Dictionary of configuration settings
    
    Returns:
        tuple: (asciidoc_report_string, structured_data_dict)
    """
    # Initialize with formatted header
    adoc_content = format_check_header(
        "Disk Usage Analysis (Shell)",
        "Checking disk space on all mounted filesystems using `df -h`.",
        requires_ssh=True
    )
    structured_data = {}
    
    # Check SSH availability
    ssh_ok, skip_msg, skip_data = require_ssh(connector, "shell commands")
    if not skip_ok:
        adoc_content.append(skip_msg)
        structured_data["disk_usage"] = skip_data
        return "\n".join(adoc_content), structured_data
    
    # Execute shell command
    query = get_disk_usage_query(connector)
    success, formatted, raw = safe_execute_query(connector, query, "df -h command")
    
    if not success:
        adoc_content.append(formatted)
        structured_data["disk_usage"] = {"status": "error", "data": raw}
        return "\n".join(adoc_content), structured_data
    
    # Shell commands return raw text output
    # The connector's shell executor may parse it into a dict with 'output' key
    output = raw.get('output', '') if isinstance(raw, dict) else str(raw)
    
    if not output:
        adoc_content.append("[NOTE]\n====\nNo disk usage data returned.\n====\n")
        structured_data["disk_usage"] = {"status": "success", "data": {}}
        return "\n".join(adoc_content), structured_data
    
    # Parse df output (line-by-line)
    # Example output:
    # Filesystem      Size  Used Avail Use% Mounted on
    # /dev/sda1       100G   75G   25G  75% /
    # /dev/sdb1       500G  450G   50G  90% /var/lib/cassandra
    
    lines = output.strip().split('\n')
    filesystems = []
    high_usage_mounts = []
    
    for line in lines[1:]:  # Skip header
        parts = line.split()
        if len(parts) >= 6:
            filesystem = parts[0]
            size = parts[1]
            used = parts[2]
            avail = parts[3]
            use_pct_str = parts[4]
            mount = parts[5]
            
            # Parse percentage
            try:
                use_pct = int(use_pct_str.rstrip('%'))
            except ValueError:
                use_pct = 0
            
            filesystems.append({
                'filesystem': filesystem,
                'size': size,
                'used': used,
                'available': avail,
                'use_percent': use_pct,
                'mounted_on': mount
            })
            
            # Flag high usage (>80%)
            if use_pct > 80:
                high_usage_mounts.append({
                    'mount': mount,
                    'use_percent': use_pct,
                    'available': avail
                })
    
    # Add formatted output
    adoc_content.append(formatted)
    
    # Analyze and report
    if high_usage_mounts:
        adoc_content.append(
            f"[WARNING]\n====\n"
            f"**{len(high_usage_mounts)} filesystem(s)** with high disk usage (>80%) detected.\n"
            "====\n"
        )
        
        # List high usage mounts
        adoc_content.append("\n==== High Usage Filesystems")
        adoc_content.append("|===\n|Mount Point|Usage %|Available")
        for mount in high_usage_mounts:
            adoc_content.append(f"|{mount['mount']}|{mount['use_percent']}%|{mount['available']}")
        adoc_content.append("|===\n")
        
        recommendations = [
            "Identify and remove old/unused data or snapshots",
            "For /var/lib/cassandra: Run 'nodetool clearsnapshot' to remove old snapshots",
            "Check for abandoned SSTables: 'find /var/lib/cassandra/data -name \"*tmp*\"'",
            "Consider adding storage capacity or archiving old data",
            "Monitor disk growth rate to predict when expansion is needed"
        ]
        adoc_content.extend(format_recommendations(recommendations))
        
        status = "warning"
    else:
        adoc_content.append(
            "[NOTE]\n====\n"
            f"All {len(filesystems)} filesystem(s) have adequate free space (<80% usage).\n"
            "====\n"
        )
        status = "success"
    
    structured_data["disk_usage"] = {
        "status": status,
        "data": filesystems,
        "high_usage_count": len(high_usage_mounts),
        "max_usage_percent": max([f['use_percent'] for f in filesystems]) if filesystems else 0
    }
    
    return "\n".join(adoc_content), structured_data
----

==== Understanding Shell Command Output Structure

Shell commands return different output formats than CQL or nodetool:

*Simple commands (df, free, etc.):*
[source,python]
----
# Raw dict with command metadata:
{
    'command': 'df -h',
    'output': 'Filesystem      Size  Used Avail Use% Mounted on\n/dev/sda1  ...',
    'stderr': None,
    'exit_code': 0
}
----

*You need to parse the 'output' field yourself* based on the command's output format.

==== Shell Command Best Practices

[TIP]
====
**When using shell commands:**

1. **Parse output manually** - Shell commands return raw text, not structured data
2. **Handle variations** - Output format can differ between Linux distributions
3. **Check exit codes** - Non-zero exit code indicates command failure
4. **Consider stderr** - Error messages appear in stderr field
5. **Be specific** - Use exact paths and options (e.g., `df -h /var/lib/cassandra`)
6. **Avoid pipes when possible** - Some shells may not support complex pipes
====

==== Common Shell Commands for Cassandra

*Disk Usage:*
```json
{"operation": "shell", "command": "df -h"}
{"operation": "shell", "command": "du -sh /var/lib/cassandra"}
```

*Memory:*
```json
{"operation": "shell", "command": "free -m"}
{"operation": "shell", "command": "cat /proc/meminfo"}
```

*Process Info:*
```json
{"operation": "shell", "command": "ps aux | grep cassandra | grep -v grep"}
{"operation": "shell", "command": "pgrep -a java"}
```

*System Load:*
```json
{"operation": "shell", "command": "uptime"}
{"operation": "shell", "command": "cat /proc/loadavg"}
```

*Network:*
```json
{"operation": "shell", "command": "netstat -s"}
{"operation": "shell", "command": "ss -s"}
```

*File System:*
```json
{"operation": "shell", "command": "find /var/lib/cassandra/data -name '*tmp*' -type f"}
{"operation": "shell", "command": "ls -lh /var/lib/cassandra/commitlog"}
```

=== Comparison: CQL vs Nodetool vs Shell

[cols="1,1,1,1"]
|===
| Aspect | CQL | Nodetool | Shell

| **Connection**
| Native Cassandra protocol
| SSH required
| SSH required

| **Output Format**
| Structured (list of dicts)
| Structured (parsed by connector)
| Raw text (manual parsing)

| **Use Cases**
| Schema, topology, replication
| Operational metrics, node status
| System-level metrics, files

| **Performance**
| Fast, direct
| Moderate (SSH overhead)
| Moderate (SSH overhead)

| **Privileges**
| CQL user permissions
| OS user with nodetool access
| OS user with shell access

| **Example Query**
| `SELECT * FROM system.local`
| `{"operation": "nodetool", "command": "status"}`
| `{"operation": "shell", "command": "df -h"}`
|===

[IMPORTANT]
====
**Choose the right tool:**

- **CQL** when data is in system tables
- **Nodetool** when you need Cassandra-specific operational metrics
- **Shell** when you need OS-level information not available through CQL or nodetool
====

=== Available System Tables for CQL

==== system.local (Single Row - Local Node Only)

*Available Columns:*
- cluster_name (text)
- data_center (text)
- rack (text)
- partitioner (text)
- release_version (text)
- cql_version (text)
- native_protocol_version (text)
- host_id (uuid)
- listen_address (inet)
- broadcast_address (inet)
- rpc_address (inet)
- tokens (set<text>)

*Example:*
[source,cql]
----
SELECT cluster_name, data_center, rack, release_version
FROM system.local;
----

==== system.peers_v2 (Cassandra 4.x+) / system.peers (3.x)

*Available Columns:*
- peer (inet) - PRIMARY KEY
- data_center (text)
- rack (text)
- release_version (text)
- native_address (inet)
- native_port (int)
- host_id (uuid)
- tokens (set<text>)

*Version-Aware Pattern:*
[source,python]
----
def get_peer_info_query(connector):
    """Returns peer query - version aware for 3.x vs 4.x."""
    if hasattr(connector, 'version_info'):
        major = connector.version_info.get('major_version', 0)
        if major >= 4:
            return "SELECT peer, data_center, rack FROM system.peers_v2;"
        else:
            return "SELECT peer, data_center, rack FROM system.peers;"
    return "SELECT peer, data_center, rack FROM system.peers_v2;"
----

==== system_schema.keyspaces

*Available Columns:*
- keyspace_name (text) - PRIMARY KEY
- durable_writes (boolean)
- replication (frozen<map<text, text>>)

*Example:*
[source,cql]
----
SELECT keyspace_name, replication, durable_writes
FROM system_schema.keyspaces;
----

==== system_schema.tables

*Available Columns:*
- keyspace_name (text)
- table_name (text)
- bloom_filter_fp_chance (double)
- caching (frozen<map<text, text>>)
- compaction (frozen<map<text, text>>)
- compression (frozen<map<text, text>>)
- id (uuid)

*Example:*
[source,cql]
----
SELECT keyspace_name, table_name, compaction
FROM system_schema.tables;
----

=== CQL Limitations and Workarounds

[CRITICAL]
====
*What CQL CANNOT Do:*

❌ `WHERE keyspace_name NOT IN (...)` - Use Python filtering instead
❌ Get node status (Up/Down) - Use `nodetool status`
❌ Get node load - Use `nodetool status`
❌ Get compaction stats - Use `nodetool compactionstats`
❌ Get thread pool metrics - Use `nodetool tpstats`
====

*Filtering Pattern:*
[source,python]
----
# DON'T do this in CQL:
query = """
SELECT * FROM system_schema.keyspaces
WHERE keyspace_name NOT IN ('system', 'system_schema');  -- ❌ Won't work
"""

# DO this in Python:
query = "SELECT * FROM system_schema.keyspaces;"
formatted, raw = connector.execute_query(query, return_raw=True)

system_keyspaces = {'system', 'system_schema', 'system_traces', 
                   'system_auth', 'system_distributed', 'system_views'}
user_keyspaces = [ks for ks in raw 
                  if ks.get('keyspace_name') not in system_keyspaces]
----

== Rule File Schema

*File:* `plugins/cassandra/rules/{check_name}.json`

[CRITICAL]
====
**File Naming Convention:**

Rule files MUST be named `{check_name}.json` to match the check file name (without `_check` suffix).

Examples:
- Check: `node_status_check.py` → Rule: `node_status.json`
- Check: `compaction_pending_tasks.py` → Rule: `compaction_pending_tasks.json`
- Check: `keyspace_replication_check.py` → Rule: `keyspace_replication.json`

**This ensures each check has its own unique rule file.**
====

[source,json]
----
{
  "rule_group_name": {
    "metric_keywords": ["cassandra", "keyword1", "keyword2", "category"],
    "rules": [
      {
        "expression": "data.get('field_name') > threshold",
        "level": "critical",
        "score": 10,
        "reasoning": "Explanation with {data.get('field')} interpolation",
        "recommendations": [
          "Action step 1",
          "Action step 2"
        ]
      }
    ]
  }
}
----

*Levels:* critical (9-10), high (7-8), medium (4-6), low (1-3)

=== Rule Example 1: Nodetool Status Check

*File:* `plugins/cassandra/rules/node_status.json`

[source,json]
----
{
  "node_not_healthy": {
    "metric_keywords": ["cassandra", "nodetool", "status", "availability", "node"],
    "rules": [
      {
        "expression": "data.get('status') != 'U' or data.get('state') != 'N'",
        "level": "critical",
        "score": 10,
        "reasoning": "Node {data.get('address')} is in state {data.get('status')}{data.get('state')} instead of UN (Up/Normal). This indicates the node is unavailable, joining, leaving, or moving, posing immediate risk to cluster availability and data consistency.",
        "recommendations": [
          "SSH to node {data.get('address')} immediately and check /var/log/cassandra/system.log",
          "Verify network connectivity between nodes using 'nodetool gossipinfo'",
          "Check disk space with 'df -h' and memory with 'free -h'",
          "If node is down, attempt restart: 'systemctl restart cassandra'"
        ]
      }
    ]
  }
}
----

=== Rule Example 2: Compaction Backlog Check

*File:* `plugins/cassandra/rules/compaction_backlog.json`

[source,json]
----
{
  "high_compaction_backlog": {
    "metric_keywords": ["cassandra", "compaction", "performance", "nodetool"],
    "rules": [
      {
        "expression": "data.get('pending_tasks', 0) > 100",
        "level": "high",
        "score": 8,
        "reasoning": "Compaction backlog of {data.get('pending_tasks')} pending tasks detected, exceeding threshold of 100. Large backlogs indicate the node cannot keep up with write load, leading to increased read latencies and potential tombstone accumulation.",
        "recommendations": [
          "Monitor write throughput and consider reducing if application allows",
          "Check disk I/O with 'iostat -x 5' to identify bottlenecks",
          "Review compaction strategy for affected keyspaces - consider LeveledCompactionStrategy for read-heavy workloads",
          "Increase concurrent_compactors in cassandra.yaml if CPU allows (default: number of disks)"
        ]
      }
    ]
  }
}
----

=== Rule Example 3: Replication Strategy Check

*File:* `plugins/cassandra/rules/keyspace_replication.json`

[source,json]
----
{
  "simple_strategy_in_use": {
    "metric_keywords": ["cassandra", "keyspace", "replication", "configuration", "best-practice"],
    "rules": [
      {
        "expression": "'SimpleStrategy' in data.get('replication', {}).get('class', '')",
        "level": "high",
        "score": 7,
        "reasoning": "Keyspace '{data.get('keyspace_name')}' uses SimpleStrategy replication. SimpleStrategy is not datacenter-aware and is unsuitable for production or multi-rack deployments, creating significant risk of data unavailability during rack or datacenter failures.",
        "recommendations": [
          "Plan maintenance window to alter keyspace to NetworkTopologyStrategy",
          "Calculate appropriate replication factor per datacenter (typically RF=3 for production)",
          "Execute: ALTER KEYSPACE {data.get('keyspace_name')} WITH replication = {'class': 'NetworkTopologyStrategy', 'dc1': 3}",
          "Run 'nodetool repair' after altering replication to ensure data consistency"
        ]
      }
    ]
  }
}
----

== AsciiDoc Formatting Rules

=== Report Structure

[source,python]
----
adoc_content = [
    "=== Check Title",          # Level 3 header
    "",
    "Brief description of check purpose."
]

# Subsections
adoc_content.append("==== Analysis Results")  # Level 4
adoc_content.append("")

# Admonition blocks
adoc_content.append("[WARNING]\n====\n**Action Required:** Issue description\n====\n")

# Data tables (from connector)
adoc_content.append(formatted)

# Recommendations
adoc_content.append("\n==== Recommendations")
adoc_content.append("[TIP]\n====\n* Best practice...\n====\n")
----

=== Admonition Types

- `[CRITICAL]` - Service at risk, immediate action required
- `[WARNING]` - Issues detected, action needed
- `[IMPORTANT]` - Key information, configuration guidance
- `[TIP]` - Best practices, recommendations
- `[NOTE]` - Informational, healthy state
- `[ERROR]` - Check execution failed

== Unit Test File (Required)

*Path:* `tests/cassandra/checks/test_{check_name}.py`

[CRITICAL]
====
**File Naming Convention:**

Test files MUST be named `test_{check_name}.py` to match the check file name.

Examples:
- Check: `node_status_check.py` → Test: `test_node_status_check.py`
- Check: `compaction_pending_tasks.py` → Test: `test_compaction_pending_tasks.py`
====

[source,python]
----
import unittest
from unittest.mock import Mock
from plugins.cassandra.checks.{check_name} import run_{check_name}, get_weight

class Test{CheckName}(unittest.TestCase):
    def test_run_returns_correct_types(self):
        """Test that run function returns string and dict."""
        mock_connector = Mock()
        mock_connector.execute_query.return_value = (
            '[NOTE]\n====\nTest\n====\n',
            []
        )
        
        result = run_{check_name}(mock_connector, {})
        
        self.assertIsInstance(result, tuple)
        self.assertEqual(len(result), 2)
        self.assertIsInstance(result[0], str)
        self.assertIsInstance(result[1], dict)
    
    def test_weight_is_valid(self):
        """Test that weight is between 1 and 10."""
        weight = get_weight()
        self.assertGreaterEqual(weight, 1)
        self.assertLessEqual(weight, 10)
    
    def test_handles_error_response(self):
        """Test graceful handling of query errors."""
        mock_connector = Mock()
        mock_connector.execute_query.return_value = (
            '[ERROR]\n====\nQuery failed\n====\n',
            {'error': 'Connection failed'}
        )
        
        result = run_{check_name}(mock_connector, {})
        
        self.assertIn('[ERROR]', result[0])
        self.assertEqual(result[1].get('status'), 'error')

if __name__ == '__main__':
    unittest.main()
----

== Output Format (CRITICAL)

[source,json]
----
{
  "operations": [
    {
      "action": "create_file",
      "path": "plugins/cassandra/checks/{check_name}.py",
      "content": "..."
    },
    {
      "action": "create_file",
      "path": "plugins/cassandra/utils/qrylib/qry_{check_name}.py",
      "content": "..."
    },
    {
      "action": "create_file",
      "path": "plugins/cassandra/rules/{check_name}.json",
      "content": "..."
    },
    {
      "action": "create_file",
      "path": "tests/cassandra/checks/test_{check_name}.py",
      "content": "..."
    }
  ],
  "integration_step": {
    "target_file_hint": "plugins/cassandra/reports/default.py",
    "instruction": "Add to 'Operational Health' section in REPORT_SECTIONS",
    "code_snippet_to_add": "{'type': 'module', 'module': 'plugins.cassandra.checks.{check_name}', 'function': 'run_{check_name}'}"
  }
}
----

== File Naming Rules (CRITICAL)

1. **Check Name Source**:
   - If the natural language request includes an explicit check name (e.g., `'keyspace_replication_health'`), this name MUST be used **exactly as provided** as the `{check_name}` for all generated files (check, query, rule, and test) without any modification, truncation, derivation, or substitution. For example, `'keyspace_replication_health'` must result in `keyspace_replication_health.py`, `qry_keyspace_replication_health.py`, `keyspace_replication_health.json`, and `test_keyspace_replication_health.py`.
   - If no explicit check name is provided, derive `{check_name}` from the natural language request by converting it to a lowercase, underscore-separated string (e.g., "check node status" → `node_status_check`), ensuring it is unique, descriptive, and avoids generic terms.
   - **CRITICAL**: Never derive a `check_name` when an explicit name is provided in the request. Derivation is only allowed when no explicit name is specified.
   - **Never use generic names** like `nodetool_queries.py`, `cassandra_rules.json`, or `check.py`, as they will be overwritten by other checks.

2. **File Naming Conventions**:
   - Check file: `plugins/cassandra/checks/{check_name}.py` (e.g., `keyspace_replication_health.py`)
   - Query file: `plugins/cassandra/utils/qrylib/qry_{check_name}.py` (e.g., `qry_keyspace_replication_health.py`)
   - Rule file: `plugins/cassandra/rules/{check_name}.json` (e.g., `keyspace_replication_health.json`)
   - Test file: `tests/cassandra/checks/test_{check_name}.py` (e.g., `test_keyspace_replication_health.py`)

3. **Consistency Enforcement**:
   - When an explicit check name is provided (e.g., `'keyspace_replication_health'`), it MUST be used verbatim for all file names and the integration step’s `code_snippet_to_add` (e.g., `{'type': 'module', 'module': 'plugins.cassandra.checks.keyspace_replication_health', 'function': 'run_keyspace_replication_health'}`).
   - Before generating the JSON plan, validate that all file paths and the integration step use the exact `{check_name}` as specified in the request or derived correctly if no explicit name is provided.
   - If the tool attempts to derive a different `check_name` (e.g., `keyspace_replication_check` instead of `keyspace_replication_health`), it MUST raise an error or warning indicating a naming mismatch and use the explicitly provided name instead.

*CRITICAL*: The module path in the integration step MUST use the full import path:
✅ `'module': 'plugins.cassandra.checks.{check_name}'`
❌ NOT: `'module': '{check_name}'`

*CRITICAL*: If an explicit check name is provided, it takes absolute precedence over any derived name. The tool MUST NOT modify, truncate, or reinterpret the provided name under any circumstances.


== Pre-Submission Validation Checklist

Before outputting JSON, verify:

✅ Query file has `__all__` list declaring all public functions

✅ Query functions with parameters provide reasonable defaults

✅ Private helper functions start with underscore (_)

✅ Chose correct tool: CQL for schema, nodetool for operations, **shell for system metrics**

✅ Used Python filtering for system keyspaces (no CQL NOT IN)

✅ **NO bind variables (?) in CQL queries - use string interpolation with function parameters**

✅ Query functions return JSON strings for nodetool/shell commands

✅ Check module handles parsed nodetool output structure

✅ **Check module manually parses shell command output**

✅ Version-aware queries check hasattr(connector, 'version_info')

✅ **RECOMMENDED: Use helper functions (require_ssh, format_check_header, etc.)**

✅ **Shell and nodetool checks use require_ssh() helper or manual SSH validation**

✅ Error handling includes SSH configuration hints for nodetool/shell

✅ Rule file has metric_keywords and proper level/score

✅ Integration step has FULL module path

✅ Unit tests cover error cases and return types

✅ **ALL FILES follow naming convention: check, qry_{check}, {check}.json, test_{check}**



== Your Task

Generate a Cassandra health check for:

*Plugin Name:* {{ plugin_name }}
*Request:* {{ natural_language_request }}

**Critical Reminders:**
1. **Choose the right tool:** CQL for schema/topology, nodetool for operations, **shell for system metrics**
2. **Filter in Python:** Never use `NOT IN` in CQL queries
3. **Use helper functions:** Prefer `require_ssh()`, `format_check_header()`, `safe_execute_query()` over manual patterns
4. **Nodetool returns structured data:** Connector parses it automatically
5. **Shell returns raw text:** You must parse the output manually
6. **Include SSH hints:** When nodetool/shell fails, suggest SSH config
7. **Format consistently:** Use `format_check_header()` and `format_recommendations()`
8. **Follow naming convention:** All files must use `{check_name}` pattern to avoid overwrites
9. **Follow the patterns:** Use the exact end-to-end examples above

**Query Type Decision Tree:**
```
Need schema info? → Use CQL
Need Cassandra operational metrics? → Use Nodetool
Need OS-level metrics? → Use Shell
```

**Preferred Pattern (Use This):**
```python
from plugins.common.check_helpers import require_ssh, format_check_header, safe_execute_query, format_recommendations

def run_{check_name}(connector, settings):
    adoc_content = format_check_header("Title", "Description", requires_ssh=True)
    ssh_ok, skip_msg, skip_data = require_ssh(connector, "nodetool")
    if not ssh_ok:
        adoc_content.append(skip_msg)
        return "\n".join(adoc_content), skip_data
    
    success, formatted, raw = safe_execute_query(connector, query, "Operation")
    if not success:
        return "\n".join(adoc_content + [formatted]), {"status": "error", "data": raw}
    
    # Analyze results...
    recommendations = ["Action 1", "Action 2"]
    adoc_content.extend(format_recommendations(recommendations))
    
    return "\n".join(adoc_content), structured_data
```

**File Naming (CRITICAL - Never Generic Names):**
- Check: `plugins/cassandra/checks/{check_name}.py`
- Query: `plugins/cassandra/utils/qrylib/qry_{check_name}.py`
- Rule: `plugins/cassandra/rules/{check_name}.json`
- Test: `tests/cassandra/checks/test_{check_name}.py`

Output ONLY the JSON plan. No explanations, no markdown, no additional text.
