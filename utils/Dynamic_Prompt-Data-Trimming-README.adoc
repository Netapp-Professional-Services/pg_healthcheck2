= Dynamic Prompt Generation and Trimming Logic
:toc: left
:toclevels: 3
:source-highlighter: rouge

An overview of the budgeting and summarization process used by `dynamic_prompt_generator.py`.

== Overview

The primary goal of the `dynamic_prompt_generator.py` script is to analyze a comprehensive JSON file of database health check findings and produce a concise, context-aware prompt for an AI model. Its most critical function is to intelligently trim down the findings to ensure the final prompt respects a configurable token limit, a process we refined to handle various scenarios correctly.

This document describes the final, corrected logic for this "balancing act."

---

== Prompt Budget Calculation

The script's behavior is driven by the `ai_max_prompt_tokens` setting. The script calculates a character budget for the main JSON data, reserving a portion of the total budget for the prompt's instructions and summaries.

[source,python]
----
# Reserve 4000 characters (~1000 tokens) for prompt instructions, headers, and summaries.
RESERVED_BUFFER_FOR_PROMPT_OVERHEAD = 4000 

max_prompt_tokens = settings.get('ai_max_prompt_tokens', 8000)
# The total budget for the entire prompt string
total_character_budget = max_prompt_tokens * TOKEN_CHARACTER_RATIO
# The budget for just the findings_json part is the total minus our buffer
token_budget = total_character_budget - RESERVED_BUFFER_FOR_PROMPT_OVERHEAD
----

This ensures the final, complete prompt sent to the AI adheres to the limit.

---

== Prioritization and Sorting

To perform the "balancing act" of including the most important information, the script uses a unified priority system.

. It calculates a **priority score** for every module.
. The base score is the module's predefined weight.
. A significant bonus is added to the score if the module contains any findings flagged as **critical** or **high** severity.
. All modules are then sorted into a single list based on this priority score, from highest to lowest.

This ensures that modules with urgent issues are always considered for inclusion first.

---

== Module Weighting: The Importance of Weights

Assigning a `weight` to each check module is a crucial configuration step that directly influences the content of the AI prompt, especially when budgets are tight.

=== Importance of Weights

The `weight` serves as the baseline priority for a module. In cases where no "critical" or "high" severity issues are found, the `weight` is the primary factor that determines the inclusion order of modules in the prompt. A module with a higher weight is considered more important and will be processed earlier in the queue. This gives it a better chance of being included in the final prompt, either in full or in a trimmed form[cite:1].

[source,python]
.Example from plugins/postgres/checks/postgres_overview.py
----
def get_weight():
    """Returns the importance score for this module."""
    return 10 # Core configuration, highest importance
----


=== Default Behavior (No Weight Defined)

If a weight is not explicitly defined for a check module, the script is designed to assign a default weight of **1**.

[source,python]
----
# The priority score calculation defaults to 1 if a weight is not found.
priority_score = module_weights.get(module_name, 1)
----

This default value effectively gives the module the lowest possible priority. As a result, it will be considered last for inclusion and is the most likely to be skipped entirely when the budget is constrained.

---

== The Trimming Process

The script iterates through the prioritized list of modules and adds them one by one, strictly enforcing the budget. For each module, it performs the following checks:

. **Check Full Size**: It first checks if the complete, untrimmed module can fit within the remaining budget. If it fits, it's added as-is, and the script moves to the next module.
. **Attempt to Trim**: If the full module does not fit, the script then attempts to trim it. Trimming involves reducing any long lists of data within the module down to a single item.
. **Check Trimmed Size**: After trimming, it checks if the new, smaller version of the module can fit in the remaining budget. If it fits, it's added, and the trimming action is logged.
. **Skip if Necessary**: If even the trimmed version of the module is too large, it is skipped entirely, and the script moves to the next module in the priority list.

This process continues until the budget is filled, guaranteeing that the most important information that can fit is always included.

---

== Behavior Scenarios

The script's output changes dramatically based on the configured `ai_max_prompt_tokens` budget.

=== Low Budget Scenario

When the token limit is small (e.g., 1000-2000 tokens), the script behaves as follows:

* The character budget is highly constrained.
* The script processes the highest-priority modules first but likely finds they are too large to be included in their untrimmed form.
* **Trimming is aggressive**: The script will immediately resort to trimming these important modules, including only their most essential data (e.g., the first item in each list).
* **Modules are skipped**: The budget will be filled quickly by just a few high-priority, trimmed modules. Most lower-priority modules will be skipped entirely.
* The console log will show a "Prompt Content Trimming Summary" and an info message stating the budget was enforced.

=== High Budget Scenario

When the token limit is large (e.g., 100,000 tokens), the behavior is much simpler:

* The character budget is very generous.
* The script processes modules in priority order, checking if the full, untrimmed version fits.
* **No trimming occurs**: Because the budget is so large, every module will fit in its original, complete form. The conditional trimming logic is never triggered.
* **All modules are included**: The final prompt sent to the AI will contain the complete, unfiltered findings data.
* The console log will not show any trimming summary, as no content was shortened.
