= Common Utilities Documentation
:toc: left
:toclevels: 3
:icons: font

== Overview

The `plugins/common` module provides shared utilities and helper functions used across all database plugins in the pg_healthcheck2 framework. These utilities promote code reuse, consistency, and maintainability by implementing the DRY (Don't Repeat Yourself) principle.

== Module Architecture

[source,text]
----
plugins/common/
├── __init__.py               # Package initialization and exports
├── ssh_handler.py            # SSH connection management
├── ssh_mixin.py              # SSH support detection for connectors
├── shell_executor.py         # Command execution utilities
├── parsers.py                # Output parsing (nodetool, shell)
├── output_formatters.py      # AsciiDoc formatting
└── check_helpers.py          # Reusable check utilities
----

== SSH Infrastructure

=== ssh_handler.py

Manages SSH connections to remote database servers using Paramiko.

==== SSHConnectionManager

Thread-safe SSH connection manager with automatic reconnection.

*Features:*
- ✅ Persistent SSH connections with keep-alive
- ✅ Automatic reconnection on connection loss
- ✅ Thread-safe connection management
- ✅ Support for key-based and password authentication
- ✅ Configurable timeouts
- ✅ Graceful connection cleanup

*Configuration:*
[source,python]
----
settings = {
    'ssh_host': 'database-server.example.com',
    'ssh_user': 'ubuntu',
    'ssh_key_file': '/home/user/.ssh/id_rsa',  # Key-based auth
    # OR
    'ssh_password': 'secret',                   # Password auth (not recommended)
    'ssh_timeout': 10                           # Connection timeout in seconds
}

ssh_manager = SSHConnectionManager(settings)
----

*Basic Usage:*
[source,python]
----
from plugins.common import SSHConnectionManager

# Initialize
ssh_manager = SSHConnectionManager(settings)

# Connect
ssh_manager.connect()

# Execute command
stdout, stderr, exit_code = ssh_manager.execute_command('df -h')

# Cleanup
ssh_manager.disconnect()
----

*With Context Manager:*
[source,python]
----
with SSHConnectionManager(settings) as ssh:
    stdout, stderr, exit_code = ssh.execute_command('nodetool status')
    if exit_code == 0:
        print(stdout)
----

*Automatic Reconnection:*
[source,python]
----
# Connection is maintained and automatically reconnected if needed
ssh_manager.ensure_connected()
stdout, stderr, exit_code = ssh_manager.execute_command('uptime')
----

*Error Handling:*
[source,python]
----
try:
    ssh_manager.connect()
except ConnectionError as e:
    logger.error(f"SSH connection failed: {e}")
    # Handle connection failure
----

*Key Methods:*

[cols="1,3"]
|===
|Method |Description

|`connect()`
|Establishes SSH connection to remote host

|`disconnect()`
|Closes SSH connection gracefully

|`ensure_connected()`
|Verifies connection is active, reconnects if needed

|`execute_command(command, timeout=30)`
|Executes command and returns (stdout, stderr, exit_code)

|`is_connected()`
|Returns True if connection is active
|===

=== ssh_mixin.py

Provides SSH capability detection for database connectors.

==== SSHSupportMixin

Mixin class that adds SSH support detection to connectors.

*Usage in Connectors:*
[source,python]
----
from plugins.common import SSHConnectionManager, SSHSupportMixin

class MyDatabaseConnector(SSHSupportMixin):
    def __init__(self, settings):
        self.settings = settings
        
        # Initialize SSH if configured
        if settings.get('ssh_host'):
            self.ssh_manager = SSHConnectionManager(settings)
    
    def connect(self):
        # Connect to database
        # ...
        
        # Connect SSH if available
        if self.has_ssh_support():
            self.ssh_manager.connect()
----

*Checking SSH Availability:*
[source,python]
----
connector = MyDatabaseConnector(settings)

if connector.has_ssh_support():
    # Execute SSH-based operations
    stdout, stderr, exit_code = connector.ssh_manager.execute_command('df -h')
else:
    # Skip SSH-based checks
    print("SSH not configured, skipping system-level checks")
----

*Key Methods:*

[cols="1,3"]
|===
|Method |Description

|`has_ssh_support()`
|Returns True if connector has SSH configured and available
|===

== Command Execution

=== shell_executor.py

Utilities for executing and processing shell commands.

==== ShellExecutor

Static utility class for shell command execution and output processing.

*Execute Command:*
[source,python]
----
from plugins.common import ShellExecutor, SSHConnectionManager

ssh_manager = SSHConnectionManager(settings)
ssh_manager.connect()

stdout, stderr, exit_code = ShellExecutor.execute_command(
    ssh_manager, 
    'df -h /var/lib/cassandra'
)

if exit_code == 0:
    print(f"Success: {stdout}")
else:
    print(f"Error: {stderr}")
----

*Parse Common Outputs:*
[source,python]
----
# Parse df (disk free) output
df_data = ShellExecutor.parse_df_output(stdout)
# Returns: [{'filesystem': '/dev/sda1', 'size': '100G', 'used': '75G', ...}]

# Parse free (memory) output
mem_data = ShellExecutor.parse_free_output(stdout)
# Returns: {'mem_total': 8192, 'mem_used': 4096, ...}

# Parse ps (process) output
processes = ShellExecutor.parse_ps_output(stdout)
# Returns: [{'user': 'cassandra', 'pid': 1234, 'cmd': '/usr/bin/java ...'}]
----

*Key Methods:*

[cols="1,2,2"]
|===
|Method |Parameters |Returns

|`execute_command(ssh_manager, command, timeout)`
|SSH manager, command string, timeout
|Tuple: (stdout, stderr, exit_code)

|`parse_df_output(output)`
|df command output
|List of filesystem dicts

|`parse_free_output(output)`
|free command output
|Dict with memory stats

|`parse_ps_output(output)`
|ps command output
|List of process dicts

|`parse_uptime_output(output)`
|uptime command output
|Dict with load averages

|`parse_netstat_output(output)`
|netstat command output
|Dict with network stats
|===

*Example Output Structures:*

*df output:*
[source,python]
----
[
    {
        'filesystem': '/dev/sda1',
        'size': '100G',
        'used': '75G',
        'available': '25G',
        'use_percent': 75,
        'mounted_on': '/'
    }
]
----

*free output:*
[source,python]
----
{
    'mem_total': 8192,      # MB
    'mem_used': 4096,
    'mem_free': 2048,
    'mem_available': 6144,
    'swap_total': 2048,
    'swap_used': 0,
    'swap_free': 2048
}
----

*ps output:*
[source,python]
----
[
    {
        'user': 'cassandra',
        'pid': 1234,
        'cpu_percent': 5.2,
        'mem_percent': 32.7,
        'command': '/usr/bin/java -Xmx4G ...'
    }
]
----

== Output Parsing

=== parsers.py

Parses nodetool and shell command outputs into structured data.

==== NodetoolParser

Parses Cassandra nodetool command outputs.

*Supported Commands:*
- `status` - Node status, load, and ownership
- `compactionstats` - Compaction statistics
- `tpstats` - Thread pool statistics
- `describecluster` - Cluster topology and schema versions
- `tablestats` - Table-level statistics
- `info` - Node information summary
- `gcstats` - Garbage collection statistics
- `proxyhistograms` - Latency histograms

*Usage:*
[source,python]
----
from plugins.common import NodetoolParser

parser = NodetoolParser()

# Parse nodetool status
status_output = """
Datacenter: datacenter1
=======================
Status=Up/Down
|/ State=Normal/Leaving/Joining/Moving
--  Address       Load       Tokens  Owns    Host ID                               Rack
UN  192.168.1.10  108.45 KB  256     33.3%   aaa-bbb-ccc                          rack1
UN  192.168.1.11  110.23 KB  256     33.3%   ddd-eee-fff                          rack1
DN  192.168.1.12  0 bytes    256     33.3%   ggg-hhh-iii                          rack1
"""

result = parser.parse('status', status_output)
# Returns list of node dicts
----

*Output Structures:*

*nodetool status:*
[source,python]
----
[
    {
        'datacenter': 'datacenter1',
        'status': 'U',              # U=Up, D=Down
        'state': 'N',               # N=Normal, L=Leaving, J=Joining, M=Moving
        'address': '192.168.1.10',
        'load': '108.45 KB',
        'tokens': 256,
        'owns_effective_percent': 33.3,
        'host_id': 'aaa-bbb-ccc',
        'rack': 'rack1'
    }
]
----

*nodetool compactionstats:*
[source,python]
----
{
    'pending_tasks': 15,
    'active_compactions': [
        {
            'compaction_id': 'abc123',
            'keyspace': 'my_keyspace',
            'table': 'my_table',
            'completed': 50000000,
            'total': 100000000,
            'unit': 'bytes',
            'type': 'Compaction',
            'progress_percent': 50.0
        }
    ]
}
----

*nodetool tpstats:*
[source,python]
----
[
    {
        'pool_name': 'ReadStage',
        'active': 0,
        'pending': 0,
        'completed': 12345,
        'blocked': 0,
        'all_time_blocked': 0
    },
    {
        'pool_name': 'MutationStage',
        'active': 2,
        'pending': 10,
        'completed': 98765,
        'blocked': 0,
        'all_time_blocked': 5
    }
]
----

*nodetool describecluster:*
[source,python]
----
{
    'name': 'Production Cluster',
    'snitch': 'org.apache.cassandra.locator.GossipingPropertyFileSnitch',
    'partitioner': 'org.apache.cassandra.dht.Murmur3Partitioner',
    'schema_versions': [
        {
            'version': '909ab78a-408f-34a2-872b-4ca50d2dfe2a',
            'endpoints': ['192.168.1.10', '192.168.1.11']
        },
        {
            'version': 'UNREACHABLE',
            'endpoints': ['192.168.1.12']
        }
    ]
}
----

*nodetool tablestats:*
[source,python]
----
{
    'keyspaces': [
        {
            'keyspace_name': 'my_keyspace',
            'read_count': 12345,
            'read_latency': 1.234,
            'write_count': 98765,
            'write_latency': 0.567,
            'pending_flushes': 0,
            'tables': [
                {
                    'table_name': 'my_table',
                    'sstable_count': 5,
                    'space_used_live': 1048576,     # bytes
                    'space_used_total': 2097152,
                    'space_used_by_snapshots_total': 524288,
                    'sstable_compression_ratio': 0.5,
                    'number_of_partitions_estimate': 1000,
                    'memtable_columns_count': 100,
                    'memtable_data_size': 65536,
                    'memtable_switch_count': 10,
                    'local_read_count': 5000,
                    'local_read_latency': 1.5,
                    'local_write_count': 8000,
                    'local_write_latency': 0.8,
                    'pending_flushes': 0,
                    'percent_repaired': 100.0,
                    'bloom_filter_false_positives': 0,
                    'bloom_filter_false_ratio': 0.0,
                    'compacted_partition_minimum_bytes': 1024,
                    'compacted_partition_maximum_bytes': 8192,
                    'compacted_partition_mean_bytes': 2048
                }
            ]
        }
    ]
}
----

*Key Methods:*

[cols="1,2,2"]
|===
|Method |Parameters |Returns

|`parse(command, output)`
|Command name, raw output
|Parsed structured data

|`_parse_status(output)`
|nodetool status output
|List of node dicts

|`_parse_compactionstats(output)`
|nodetool compactionstats output
|Dict with compaction info

|`_parse_tpstats(output)`
|nodetool tpstats output
|List of thread pool dicts

|`_parse_describecluster(output)`
|nodetool describecluster output
|Dict with cluster info

|`_parse_tablestats(output)`
|nodetool tablestats output
|Dict with keyspace/table stats
|===

*Error Handling:*
[source,python]
----
try:
    result = parser.parse('status', output)
except ValueError as e:
    logger.error(f"Parse error: {e}")
    result = []
----

== Output Formatting

=== output_formatters.py

Formats data into AsciiDoc markup for health check reports.

==== AsciiDocFormatter

Formats various data types into AsciiDoc tables and admonitions.

*Format Query Results:*
[source,python]
----
from plugins.common import AsciiDocFormatter

formatter = AsciiDocFormatter()

# Format list of dicts as table
data = [
    {'name': 'keyspace1', 'replication_factor': 3},
    {'name': 'keyspace2', 'replication_factor': 1}
]

table = formatter.format_table(data)
print(table)
----

*Output:*
[source,asciidoc]
----
|===
|name|replication_factor
|keyspace1|3
|keyspace2|1
|===
----

*Format Nodetool Output:*
[source,python]
----
# Format nodetool status
nodes = [
    {'datacenter': 'dc1', 'status': 'U', 'state': 'N', 'address': '192.168.1.10'},
    {'datacenter': 'dc1', 'status': 'D', 'state': 'N', 'address': '192.168.1.11'}
]

formatted = formatter.format_nodetool_status(nodes)
----

*Format Shell Output:*
[source,python]
----
# Format shell command output
formatted = formatter.format_shell_output('df -h', stdout)
----

*Format Admonitions:*
[source,python]
----
# Format different message types
note = formatter.format_note("Everything is healthy")
warning = formatter.format_warning("High disk usage detected")
critical = formatter.format_critical("Node is down!")
error = formatter.format_error("Query failed")
tip = formatter.format_tip("Run nodetool repair")
----

*Output:*
[source,asciidoc]
----
[NOTE]
====
Everything is healthy
====

[WARNING]
====
High disk usage detected
====

[CRITICAL]
====
Node is down!
====

[ERROR]
====
Query failed
====

[TIP]
====
Run nodetool repair
====
----

*Key Methods:*

[cols="1,2,2"]
|===
|Method |Parameters |Returns

|`format_table(data)`
|List of dicts or list of lists
|AsciiDoc table string

|`format_nodetool_status(nodes)`
|List of node dicts
|Formatted table string

|`format_nodetool_compactionstats(stats)`
|Compaction stats dict
|Formatted output string

|`format_nodetool_tpstats(pools)`
|List of thread pool dicts
|Formatted table string

|`format_shell_output(command, output)`
|Command name and output
|Formatted code block

|`format_note(message)`
|Message string
|AsciiDoc NOTE admonition

|`format_warning(message)`
|Message string
|AsciiDoc WARNING admonition

|`format_critical(message)`
|Message string
|AsciiDoc CRITICAL admonition

|`format_error(message)`
|Message string
|AsciiDoc ERROR admonition

|`format_tip(message)`
|Message string
|AsciiDoc TIP admonition
|===

== Check Helpers

=== check_helpers.py

Reusable helper functions that reduce boilerplate in health check modules.

==== require_ssh()

Checks if SSH is configured and available for the connector.

*Usage:*
[source,python]
----
from plugins.common.check_helpers import require_ssh

def run_my_check(connector, settings):
    adoc_content = []
    structured_data = {}
    
    # Check SSH availability
    ssh_ok, skip_msg, skip_data = require_ssh(connector, "nodetool commands")
    if not ssh_ok:
        adoc_content.append(skip_msg)
        structured_data["result"] = skip_data
        return "\n".join(adoc_content), structured_data
    
    # SSH is available, proceed with check
    # ...
----

*Returns:*
[source,python]
----
(
    True,                           # SSH is available
    "",                            # Empty skip message
    {}                             # Empty skip data
)
# OR
(
    False,                          # SSH not available
    "[IMPORTANT]\n====\n...",      # Skip message (formatted AsciiDoc)
    {                              # Skip data (structured)
        "status": "skipped",
        "reason": "SSH not configured"
    }
)
----

==== format_check_header()

Creates standardized check headers with optional SSH requirement notice.

*Usage:*
[source,python]
----
from plugins.common.check_helpers import format_check_header

# Without SSH requirement
adoc_content = format_check_header(
    "Keyspace Replication Analysis",
    "Analyzing replication strategies for all user keyspaces."
)

# With SSH requirement
adoc_content = format_check_header(
    "Node Status Check (Nodetool)",
    "Checking cluster node health using `nodetool status`.",
    requires_ssh=True
)
----

*Output:*
[source,python]
----
[
    "=== Node Status Check (Nodetool)",
    "",
    "Checking cluster node health using `nodetool status`.",
    "",
    "[NOTE]",
    "====",
    "This check requires SSH access to the database server.",
    "===="
]
----

==== format_recommendations()

Formats recommendation lists consistently with proper AsciiDoc markup.

*Usage:*
[source,python]
----
from plugins.common.check_helpers import format_recommendations

recommendations = [
    "SSH to affected nodes and check logs",
    "Verify network connectivity between nodes",
    "Run 'nodetool repair' to ensure data consistency"
]

adoc_content.extend(format_recommendations(recommendations))
----

*Output:*
[source,asciidoc]
----
==== Recommendations
[TIP]
====
* SSH to affected nodes and check logs
* Verify network connectivity between nodes
* Run 'nodetool repair' to ensure data consistency
====
----

==== safe_execute_query()

Wraps query execution with consistent error handling and logging.

*Usage:*
[source,python]
----
from plugins.common.check_helpers import safe_execute_query

def run_my_check(connector, settings):
    query = get_my_query(connector)
    
    # Execute with error handling
    success, formatted, raw = safe_execute_query(
        connector, 
        query, 
        "My query description"
    )
    
    if not success:
        # Query failed, formatted contains error message
        adoc_content.append(formatted)
        structured_data["result"] = {"status": "error", "data": raw}
        return "\n".join(adoc_content), structured_data
    
    # Query succeeded, process results
    # formatted = AsciiDoc formatted output
    # raw = structured data (list/dict)
----

*Returns:*
[source,python]
----
(
    True,                           # Success
    "[AsciiDoc table...]",         # Formatted output
    [{'col1': 'val1', ...}]        # Raw structured data
)
# OR
(
    False,                          # Failure
    "[ERROR]\n====\n...",          # Error message (formatted)
    {'error': 'details'}           # Error info
)
----

*Benefits:*
- ✅ Consistent error message formatting
- ✅ Automatic exception handling
- ✅ Logging of errors
- ✅ Clean success/failure checking

==== Complete Check Example

Here's a complete check using all helpers:

[source,python]
----
from plugins.cassandra.utils.qrylib.qry_node_status import get_nodetool_status_query
from plugins.common.check_helpers import (
    require_ssh,
    format_check_header,
    format_recommendations,
    safe_execute_query
)

def get_weight():
    return 9  # Critical check

def run_node_status_check(connector, settings):
    """Check node status using nodetool."""
    
    # 1. Initialize with formatted header
    adoc_content = format_check_header(
        "Node Status Analysis (Nodetool)",
        "Checking cluster node health using `nodetool status`.",
        requires_ssh=True
    )
    structured_data = {}
    
    # 2. Check SSH availability
    ssh_ok, skip_msg, skip_data = require_ssh(connector, "nodetool commands")
    if not ssh_ok:
        adoc_content.append(skip_msg)
        structured_data["node_status"] = skip_data
        return "\n".join(adoc_content), structured_data
    
    # 3. Execute query safely
    query = get_nodetool_status_query(connector)
    success, formatted, raw = safe_execute_query(connector, query, "Nodetool status")
    
    if not success:
        adoc_content.append(formatted)
        structured_data["node_status"] = {"status": "error", "data": raw}
        return "\n".join(adoc_content), structured_data
    
    # 4. Analyze results
    nodes = raw if isinstance(raw, list) else []
    unhealthy_nodes = [n for n in nodes if n['status'] != 'U' or n['state'] != 'N']
    
    # 5. Format results
    if unhealthy_nodes:
        adoc_content.append(
            f"[CRITICAL]\n====\n"
            f"**{len(unhealthy_nodes)} node(s)** not in UN state.\n"
            "====\n"
        )
        adoc_content.append(formatted)
        
        # 6. Add recommendations using helper
        recommendations = [
            "SSH to affected nodes and check /var/log/cassandra/system.log",
            "Verify network connectivity between nodes",
            "Check disk space with 'df -h'",
            "If node is down, restart: 'systemctl restart cassandra'"
        ]
        adoc_content.extend(format_recommendations(recommendations))
    else:
        adoc_content.append(
            f"[NOTE]\n====\n"
            f"All {len(nodes)} node(s) are healthy (UN state).\n"
            "====\n"
        )
    
    # 7. Return results
    structured_data["node_status"] = {
        "status": "success",
        "data": nodes,
        "total_nodes": len(nodes),
        "unhealthy_count": len(unhealthy_nodes)
    }
    
    return "\n".join(adoc_content), structured_data
----

== Package Initialization

=== __init__.py

The `__init__.py` file exports all common utilities for easy importing.

*Available Exports:*
[source,python]
----
from plugins.common import (
    # SSH Infrastructure
    SSHConnectionManager,
    SSHSupportMixin,
    
    # Command Execution
    ShellExecutor,
    
    # Parsers
    NodetoolParser,
    
    # Formatters
    AsciiDocFormatter,
    
    # Check Helpers
    require_ssh,
    format_check_header,
    format_recommendations,
    safe_execute_query
)
----

*Usage Example:*
[source,python]
----
# Import everything you need in one line
from plugins.common import (
    SSHConnectionManager,
    NodetoolParser,
    AsciiDocFormatter,
    require_ssh,
    safe_execute_query
)

# Use the utilities
ssh = SSHConnectionManager(settings)
parser = NodetoolParser()
formatter = AsciiDocFormatter()
----

== Best Practices

=== SSH Connection Management

*Do:*
[source,python]
----
# ✅ Use context manager for automatic cleanup
with SSHConnectionManager(settings) as ssh:
    stdout, stderr, exit_code = ssh.execute_command('df -h')

# ✅ Check SSH availability before using
if connector.has_ssh_support():
    # Use SSH features
    pass
else:
    # Skip SSH-dependent checks
    pass

# ✅ Use ensure_connected() for long-running operations
ssh_manager.ensure_connected()
result = ssh_manager.execute_command('nodetool repair')
----

*Don't:*
[source,python]
----
# ❌ Don't leave connections open
ssh = SSHConnectionManager(settings)
ssh.connect()
# ... forgot to disconnect()

# ❌ Don't assume SSH is always available
stdout, stderr, exit_code = connector.ssh_manager.execute_command('df -h')
# This will fail if SSH isn't configured

# ❌ Don't ignore connection errors
ssh.connect()  # No error handling
----

=== Parser Usage

*Do:*
[source,python]
----
# ✅ Always handle parse errors
try:
    result = parser.parse('status', output)
except ValueError as e:
    logger.error(f"Parse error: {e}")
    result = []

# ✅ Validate parsed data structure
if isinstance(result, list) and len(result) > 0:
    # Process results
    pass

# ✅ Use appropriate parser for command type
nodetool_parser = NodetoolParser()
result = nodetool_parser.parse('status', output)
----

*Don't:*
[source,python]
----
# ❌ Don't assume parsing always succeeds
result = parser.parse('status', output)
first_node = result[0]  # May fail if result is empty

# ❌ Don't use wrong parser
shell_output = ssh.execute_command('df -h')
result = nodetool_parser.parse('df', shell_output)  # Wrong parser!
----

=== Check Helper Usage

*Do:*
[source,python]
----
# ✅ Always use require_ssh() for SSH-dependent checks
ssh_ok, skip_msg, skip_data = require_ssh(connector, "operation")
if not ssh_ok:
    return skip_msg, skip_data

# ✅ Use format_check_header() for consistency
adoc_content = format_check_header("Title", "Description", requires_ssh=True)

# ✅ Use safe_execute_query() for error handling
success, formatted, raw = safe_execute_query(connector, query, "Description")
if not success:
    return formatted, {"status": "error", "data": raw}

# ✅ Use format_recommendations() for consistent formatting
adoc_content.extend(format_recommendations(["Step 1", "Step 2"]))
----

*Don't:*
[source,python]
----
# ❌ Don't manually check SSH
if 'ssh_host' in settings:  # Wrong - checks wrong settings
    # ...

# ❌ Don't manually format headers
adoc_content = ["=== My Check", "", "Description"]  # Inconsistent

# ❌ Don't handle errors manually
try:
    formatted, raw = connector.execute_query(query, return_raw=True)
except Exception as e:
    # Manual error handling - inconsistent
    pass
----

=== Formatter Usage

*Do:*
[source,python]
----
# ✅ Use appropriate formatter for data type
table = formatter.format_table(list_of_dicts)
adoc_content.append(table)

# ✅ Use admonitions for different message types
adoc_content.append(formatter.format_critical("Critical issue!"))
adoc_content.append(formatter.format_warning("Warning message"))
adoc_content.append(formatter.format_note("Informational note"))

# ✅ Format shell output consistently
formatted = formatter.format_shell_output('df -h', stdout)
----

*Don't:*
[source,python]
----
# ❌ Don't manually create tables
table = "|===\n|col1|col2\n"  # Inconsistent, error-prone

# ❌ Don't mix formatting styles
adoc_content.append("[WARNING]\n====\nMessage\n====\n")  # Manual
adoc_content.append(formatter.format_warning("Message"))  # Helper
# Choose one approach and stick with it

# ❌ Don't assume data structure
table = formatter.format_table(data)  # May fail if data is None
----

== Error Handling Patterns

=== SSH Errors

[source,python]
----
try:
    ssh_manager.connect()
except ConnectionError as e:
    logger.error(f"SSH connection failed: {e}")
    return error_response("SSH connection failed")
except AuthenticationError as e:
    logger.error(f"SSH authentication failed: {e}")
    return error_response("Check SSH credentials")
except TimeoutError as e:
    logger.error(f"SSH connection timeout: {e}")
    return error_response("SSH connection timeout")
----

=== Parser Errors

[source,python]
----
try:
    result = parser.parse('status', output)
except ValueError as e:
    logger.error(f"Parse error: {e}")
    result = []
except KeyError as e:
    logger.error(f"Missing expected field: {e}")
    result = []
----

=== Query Execution Errors

[source,python]
----
# Using safe_execute_query (recommended)
success, formatted, raw = safe_execute_query(connector, query, "Operation")
if not success:
    # Error already logged and formatted
    return formatted, {"status": "error", "data": raw}

# Manual approach (if needed)
try:
    formatted, raw = connector.execute_query(query, return_raw=True)
except Exception as e:
    logger.error(f"Query failed: {e}", exc_info=True)
    error_msg = formatter.format_error(f"Query failed: {str(e)}")
    return error_msg, {"status": "error", "details": str(e)}
----

== Testing Utilities

=== Unit Testing with Mocks

[source,python]
----
import unittest
from unittest.mock import Mock, patch
from plugins.common import SSHConnectionManager, NodetoolParser

class TestMyCheck(unittest.TestCase):
    def test_ssh_connection(self):
        """Test SSH connection management."""
        mock_settings = {
            'ssh_host': 'localhost',
            'ssh_user': 'test',
            'ssh_key_file': '/path/to/key'
        }
        
        with patch('paramiko.SSHClient') as mock_ssh:
            ssh = SSHConnectionManager(mock_settings)
            ssh.connect()
            
            self.assertTrue(mock_ssh.called)
    
    def test_parser(self):
        """Test nodetool output parsing."""
        parser = NodetoolParser()
        output = "UN  192.168.1.10  100KB  256  33.3%  aaa-bbb  rack1"
        
        result = parser.parse('status', output)
        
        self.assertIsInstance(result, list)
        self.assertEqual(len(result), 1)
        self.assertEqual(result[0]['status'], 'U')
----

=== Integration Testing

[source,python]
----
from plugins.common import SSHConnectionManager, ShellExecutor

def test_ssh_integration():
    """Test actual SSH connection (requires test environment)."""
    settings = {
        'ssh_host': 'test-server',
        'ssh_user': 'testuser',
        'ssh_key_file': '/path/to/test/key'
    }
    
    with SSHConnectionManager(settings) as ssh:
        stdout, stderr, exit_code = ssh.execute_command('echo "test"')
        
        assert exit_code == 0
        assert stdout.strip() == "test"
        assert stderr == ""
----

== Performance Considerations

=== Connection Pooling

*Current Implementation:*
- One SSH connection per connector instance
- Connection is kept alive with keep-alive packets
- Automatic reconnection on connection loss

*Optimization Tips:*
[source,python]
----
# ✅ Reuse connector instance across multiple checks
connector = CassandraConnector(settings)
connector.connect()

for check in checks:
    result = check(connector, settings)  # Reuses same SSH connection

connector.disconnect()

# ❌ Don't create new connector for each check
for check in checks:
    connector = CassandraConnector(settings)  # New SSH connection each time
    connector.connect()
    result = check(connector, settings)
    connector.disconnect()
----

=== Parser Performance

*Optimization Tips:*
[source,python]
----
# ✅ Parse once, use multiple times
output = ssh.execute_command('nodetool status')[0]
parsed = parser.parse('status', output)

unhealthy = [n for n in parsed if n['status'] != 'U']
high_load = [n for n in parsed if parse_load(n['load']) > threshold]

# ❌ Don't parse multiple times
output = ssh.execute_command('nodetool status')[0]
unhealthy = parser.parse('status', output)  # Parse
output = ssh.execute_command('nodetool status')[0]  # Execute again
high_load = parser.parse('status', output)  # Parse again
----

== Troubleshooting

=== Common Issues

*SSH Connection Failures:*
[source,text]
----
Problem: Connection refused or timeout
Solution: Verify SSH port is open, check firewall rules

Problem: Authentication failed
Solution: Verify SSH key permissions (chmod 600), check username

Problem: Connection drops frequently
Solution: Increase keep-alive interval, check network stability
----

*Parser Failures:*
[source,text]
----
Problem: Parse returns empty results
Solution: Check output format hasn't changed, verify command succeeded

Problem: KeyError during parsing
Solution: Cassandra version may have changed output format, update parser

Problem: Unexpected data structure
Solution: Enable debug logging to see raw output, adjust parser
----

*Formatter Issues:*
[source,text]
----
Problem: Tables not rendering correctly
Solution: Verify data is list of dicts with consistent keys

Problem: Admonitions not showing
Solution: Check AsciiDoc syntax, ensure proper spacing

Problem: Special characters breaking output
Solution: Escape special AsciiDoc characters (|, [, ])
----

== Version Compatibility

*Python:*
- ✅ Tested with Python 3.8+
- ✅ Compatible with Python 3.10+

*Dependencies:*
- Paramiko 2.x+ (SSH connections)
- Standard library only for other modules

*Cassandra Versions:*
- ✅ Nodetool parsers support Cassandra 3.x and 4.x
- ⚠️ Some output formats differ between versions
- 💡 Use version detection in connectors for compatibility

== Migration Guide

=== From Manual SSH to SSHConnectionManager

*Before:*
[source,python]
----
import paramiko

ssh_client = paramiko.SSHClient()
ssh_client.set_missing_host_key_policy(paramiko.AutoAddPolicy())
ssh_client.connect(
    hostname=settings['ssh_host'],
    username=settings['ssh_user'],
    key_filename=settings['ssh_key_file']
)

stdin, stdout, stderr = ssh_client.exec_command('df -h')
output = stdout.read().decode()

ssh_client.close()
----

*After:*
[source,python]
----
from plugins.common import SSHConnectionManager

with SSHConnectionManager(settings) as ssh:
    stdout, stderr, exit_code = ssh.execute_command('df -h')
    # output is already decoded
----

=== From Manual Parsing to NodetoolParser

*Before:*
[source,python]
----
output = execute_nodetool('status')
lines = output.split('\n')
nodes = []

for line in lines:
    if line.startswith('UN') or line.startswith('DN'):
        parts = line.split()
        nodes.append({
            'status': parts[0][0],
            'state': parts[0][1],
            'address': parts[1],
            # ... manual parsing
        })
----

*After:*
[source,python]
----
from plugins.common import NodetoolParser

parser = NodetoolParser()
output = execute_nodetool('status')
nodes = parser.parse('status', output)  # Fully parsed
----

=== From Manual Formatting to Helpers

*Before:*
[source,python]
----
adoc_content = [
    "=== My Check",
    "",
    "Description of check",
    "",
    "[NOTE]",
    "====",
    "This check requires SSH access to the database server.",
    "===="
]

# ... check logic ...

if issues:
    adoc_content.append("\n==== Recommendations")
    adoc_content.append("[TIP]\n====")
    adoc_content.append("* Fix issue 1")
    adoc_content.append("* Fix issue 2")
    adoc_content.append("====\n")
----

*After:*
[source,python]
----
from plugins.common.check_helpers import (
    format_check_header,
    format_recommendations
)

adoc_content = format_check_header(
    "My Check",
    "Description of check",
    requires_ssh=True
)

# ... check logic ...

if issues:
    recommendations = ["Fix issue 1", "Fix issue 2"]
    adoc_content.extend(format_recommendations(recommendations))
----

== Contributing

When adding new utilities to `plugins/common`:

1. **Follow existing patterns** - Look at similar utilities
2. **Add comprehensive docstrings** - Include usage examples
3. **Write unit tests** - Test all public methods
4. **Update this README** - Document new utilities
5. **Export in __init__.py** - Make utilities easily accessible

== Additional Resources

*Related Documentation:*
- Plugin-specific READMEs (e.g., `plugins/cassandra/README.adoc`)
- Main framework documentation (`README.adoc`)
- Check generation prompt (`tools/templates/check_generation/cassandra_check_prompt.adoc`)

*External References:*
- http://www.paramiko.org/[Paramiko Documentation]
- https://docs.python.org/3/library/logging.html[Python Logging]
- https://asciidoc.org/[AsciiDoc Documentation]

== License

This module is part of the pg_healthcheck2 framework.
